{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset('rajpurkar/squad_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for train split:\n",
      "Number of samples: 130319\n",
      "Total number of tokens: 2082288\n",
      "Average tokens per sample: 15.978391485508636\n",
      "Average tokens per question: 12.582186787805309\n",
      "Average tokens per answer: 3.396204697703328\n",
      "Standard deviation of tokens per sample: 6.075638479438013\n",
      "\n",
      "\n",
      "Statistics for validation split:\n",
      "Number of samples: 11873\n",
      "Total number of tokens: 179075\n",
      "Average tokens per sample: 15.082540217299755\n",
      "Average tokens per question: 12.580308262444202\n",
      "Average tokens per answer: 2.5022319548555547\n",
      "Standard deviation of tokens per sample: 5.67542834227307\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################ for Qwen2.5 ##########################\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    question_tokens_list = []\n",
    "    answer_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        question = example['question']\n",
    "        answers = example['answers']['text'] \n",
    "        if answers: \n",
    "            answer = answers[0] \n",
    "        else:\n",
    "            answer = \"\"\n",
    "        \n",
    "        question_tokens = tokenize_and_count(question)\n",
    "        answer_tokens = tokenize_and_count(answer)\n",
    "        total_tokens += question_tokens + answer_tokens\n",
    "        question_tokens_list.append(question_tokens)\n",
    "        answer_tokens_list.append(answer_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_question_tokens = np.mean(question_tokens_list)\n",
    "    average_answer_tokens = np.mean(answer_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([q + a for q, a in zip(question_tokens_list, answer_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per question\": average_question_tokens,\n",
    "        \"Average tokens per answer\": average_answer_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for split in dataset.keys():\n",
    "    results[split] = compute_statistics(dataset[split])\n",
    "\n",
    "for split, stats in results.items():\n",
    "    print(f\"Statistics for {split} split:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for train split:\n",
      "Number of samples: 130319\n",
      "Total number of tokens: 2024789\n",
      "Average tokens per sample: 15.53717416493374\n",
      "Average tokens per question: 12.403678665428679\n",
      "Average tokens per answer: 3.133495499505061\n",
      "Standard deviation of tokens per sample: 6.515199273577803\n",
      "\n",
      "\n",
      "Statistics for validation split:\n",
      "Number of samples: 11873\n",
      "Total number of tokens: 174645\n",
      "Average tokens per sample: 14.709424745220248\n",
      "Average tokens per question: 12.376400235829193\n",
      "Average tokens per answer: 2.3330245093910555\n",
      "Standard deviation of tokens per sample: 5.432816823853024\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################ for Pythia-2.8b ##########################\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    question_tokens_list = []\n",
    "    answer_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        question = example['question']\n",
    "        answers = example['answers']['text'] \n",
    "        if answers: \n",
    "            answer = answers[0] \n",
    "        else:\n",
    "            answer = \"\"\n",
    "        \n",
    "        question_tokens = tokenize_and_count(question)\n",
    "        answer_tokens = tokenize_and_count(answer)\n",
    "        total_tokens += question_tokens + answer_tokens\n",
    "        question_tokens_list.append(question_tokens)\n",
    "        answer_tokens_list.append(answer_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_question_tokens = np.mean(question_tokens_list)\n",
    "    average_answer_tokens = np.mean(answer_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([q + a for q, a in zip(question_tokens_list, answer_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per question\": average_question_tokens,\n",
    "        \"Average tokens per answer\": average_answer_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for split in dataset.keys():\n",
    "    results[split] = compute_statistics(dataset[split])\n",
    "\n",
    "for split, stats in results.items():\n",
    "    print(f\"Statistics for {split} split:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caec1f7ef0c4f0bad5545aec88a90ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e7eec2152c4d30bbf464b8df36291b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e715f1215c4c8eb531e61018df94ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c31061503274d5f8e76e23e5eb9dd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db6494e20bf40179485acb600b82cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/55.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de164226b6db4fcea48ead3b3958369c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13aac62f6d864103b25aa2796e8ce607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc34cf2da574d518340a387f3fb544c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/138384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152f4a8dfcc3482f96334759e663a33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/17944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465135ba252e434e9a2d9cb5ea9286e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/17210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aff806dc85490a8e3f1a9d01fb5f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426602286424f56ac17ee7b8df12385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1d09d7315d45149409dd745642595b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/47 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475000ab28d84ab8b5761d5a8e748841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed5a480167342a69e3fafe71b3b7f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5317485a5324c6c81474dd531cac61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97da8f33a3a0438ba0bbbbd9a248d466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/226M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b247d4ca374dcb95df5fb091a79199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff907b5a20b34ce599eb159cd7357cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c012fead754ee0842664fc6af987d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/229M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5094244d6b16445c8b4727d3c7ce6ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/235M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d212ae359a2946a6bbae573f40aa5c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce39b50727a84a9ab391bc5c39797dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d42e376c41c4d5fa9098ce1ef7259bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/266M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2549598519db4db2902ecab2c92d9147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/295M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707c12eb5f1f474bb754c8bb1e9b10e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101bd1775d26497baf4ab1d4f71eff64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8f3f4d1d7e4e4cb03213d5398655df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4942fc40d7914022aa24c2c9ebadf746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd3d2fd191644cc8e91e194f57d7595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9223851d144241eb811b3efbd626ce28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6082fd233c44339ec0375679bb04bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/33.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3610d0a27845d08dfa44f44504d8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54200d06f4c44ea960f4ee0c33f5a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/762k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249426e55b984f39aeda3e6ed19d9bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3cfeb2cc34ce29f2c201a9a55ea38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ef0f0f2d894e8dbecebb0f0ffc3298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for configuration: rc\n",
      "  Split: train\n",
      "    Number of samples: 138384\n",
      "    Total number of tokens: 3387283\n",
      "    Average tokens per sample: 24.477417909584922\n",
      "    Average tokens per question: 20.75877269048445\n",
      "    Average tokens per answer: 3.718645219100474\n",
      "    Standard deviation of tokens per sample: 11.268288609224047\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 17944\n",
      "    Total number of tokens: 439487\n",
      "    Average tokens per sample: 24.492142220240748\n",
      "    Average tokens per question: 20.784106107891215\n",
      "    Average tokens per answer: 3.708036112349532\n",
      "    Standard deviation of tokens per sample: 11.268767206697019\n",
      "\n",
      "  Split: test\n",
      "    Number of samples: 17210\n",
      "    Total number of tokens: 375978\n",
      "    Average tokens per sample: 21.846484601975597\n",
      "    Average tokens per question: 20.846484601975597\n",
      "    Average tokens per answer: 1.0\n",
      "    Standard deviation of tokens per sample: 10.875440809538137\n",
      "\n",
      "Results for configuration: rc.nocontext\n",
      "  Split: train\n",
      "    Number of samples: 138384\n",
      "    Total number of tokens: 3387283\n",
      "    Average tokens per sample: 24.477417909584922\n",
      "    Average tokens per question: 20.75877269048445\n",
      "    Average tokens per answer: 3.718645219100474\n",
      "    Standard deviation of tokens per sample: 11.268288609224047\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 17944\n",
      "    Total number of tokens: 439487\n",
      "    Average tokens per sample: 24.492142220240748\n",
      "    Average tokens per question: 20.784106107891215\n",
      "    Average tokens per answer: 3.708036112349532\n",
      "    Standard deviation of tokens per sample: 11.268767206697019\n",
      "\n",
      "  Split: test\n",
      "    Number of samples: 17210\n",
      "    Total number of tokens: 375978\n",
      "    Average tokens per sample: 21.846484601975597\n",
      "    Average tokens per question: 20.846484601975597\n",
      "    Average tokens per answer: 1.0\n",
      "    Standard deviation of tokens per sample: 10.875440809538137\n",
      "\n",
      "Results for configuration: unfiltered\n",
      "  Split: train\n",
      "    Number of samples: 87622\n",
      "    Total number of tokens: 2208574\n",
      "    Average tokens per sample: 25.20570176439707\n",
      "    Average tokens per question: 20.956380817602884\n",
      "    Average tokens per answer: 4.249320946794184\n",
      "    Standard deviation of tokens per sample: 11.864747408885416\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 11313\n",
      "    Total number of tokens: 284396\n",
      "    Average tokens per sample: 25.138866790418103\n",
      "    Average tokens per question: 20.948819941660037\n",
      "    Average tokens per answer: 4.190046848758066\n",
      "    Standard deviation of tokens per sample: 11.871087971503579\n",
      "\n",
      "  Split: test\n",
      "    Number of samples: 10832\n",
      "    Total number of tokens: 238519\n",
      "    Average tokens per sample: 22.019848596750368\n",
      "    Average tokens per question: 21.019848596750368\n",
      "    Average tokens per answer: 1.0\n",
      "    Standard deviation of tokens per sample: 11.105529501174146\n",
      "\n",
      "Results for configuration: unfiltered.nocontext\n",
      "  Split: train\n",
      "    Number of samples: 87622\n",
      "    Total number of tokens: 2208574\n",
      "    Average tokens per sample: 25.20570176439707\n",
      "    Average tokens per question: 20.956380817602884\n",
      "    Average tokens per answer: 4.249320946794184\n",
      "    Standard deviation of tokens per sample: 11.864747408885416\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 11313\n",
      "    Total number of tokens: 284396\n",
      "    Average tokens per sample: 25.138866790418103\n",
      "    Average tokens per question: 20.948819941660037\n",
      "    Average tokens per answer: 4.190046848758066\n",
      "    Standard deviation of tokens per sample: 11.871087971503579\n",
      "\n",
      "  Split: test\n",
      "    Number of samples: 10832\n",
      "    Total number of tokens: 238519\n",
      "    Average tokens per sample: 22.019848596750368\n",
      "    Average tokens per question: 21.019848596750368\n",
      "    Average tokens per answer: 1.0\n",
      "    Standard deviation of tokens per sample: 11.105529501174146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    question_tokens_list = []\n",
    "    answer_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        question = example['question']\n",
    "        answers = example['answer']['value'] \n",
    "        if isinstance(answers, list): \n",
    "            answer = answers[0] if answers else \"\"\n",
    "        else:\n",
    "            answer = answers\n",
    "\n",
    "        question_tokens = tokenize_and_count(question)\n",
    "        answer_tokens = tokenize_and_count(answer)\n",
    "        total_tokens += question_tokens + answer_tokens\n",
    "        question_tokens_list.append(question_tokens)\n",
    "        answer_tokens_list.append(answer_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_question_tokens = np.mean(question_tokens_list)\n",
    "    average_answer_tokens = np.mean(answer_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([q + a for q, a in zip(question_tokens_list, answer_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per question\": average_question_tokens,\n",
    "        \"Average tokens per answer\": average_answer_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "configs = [\"rc\", \"rc.nocontext\", \"unfiltered\", \"unfiltered.nocontext\"]\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    dataset = load_dataset(\"mandarjoshi/trivia_qa\", config)\n",
    "    config_results = {}\n",
    "    for split in dataset.keys():\n",
    "        config_results[split] = compute_statistics(dataset[split])\n",
    "    results[config] = config_results\n",
    "\n",
    "for config, splits in results.items():\n",
    "    print(f\"Results for configuration: {config}\")\n",
    "    for split, stats in splits.items():\n",
    "        print(f\"  Split: {split}\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for configuration: mlqa.en.en\n",
      "  Split: test\n",
      "    Number of samples: 11590\n",
      "    Total number of tokens: 184175\n",
      "    Average tokens per sample: 15.890854184641933\n",
      "    Average tokens per question: 10.623468507333909\n",
      "    Average tokens per answer: 5.267385677308024\n",
      "    Standard deviation of tokens per sample: 6.207366622975092\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 1148\n",
      "    Total number of tokens: 18292\n",
      "    Average tokens per sample: 15.933797909407666\n",
      "    Average tokens per question: 10.85191637630662\n",
      "    Average tokens per answer: 5.081881533101045\n",
      "    Standard deviation of tokens per sample: 6.0972782413295485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    question_tokens_list = []\n",
    "    answer_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        question = example['question']\n",
    "        answer = example['answers']['text'][0] \n",
    "        question_tokens = tokenize_and_count(question)\n",
    "        answer_tokens = tokenize_and_count(answer)\n",
    "        total_tokens += question_tokens + answer_tokens\n",
    "        question_tokens_list.append(question_tokens)\n",
    "        answer_tokens_list.append(answer_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_question_tokens = np.mean(question_tokens_list)\n",
    "    average_answer_tokens = np.mean(answer_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([q + a for q, a in zip(question_tokens_list, answer_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per question\": average_question_tokens,\n",
    "        \"Average tokens per answer\": average_answer_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "# Available configurations\n",
    "configs = [ 'mlqa.en.en']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    dataset = load_dataset(\"facebook/mlqa\", config)\n",
    "    config_results = {}\n",
    "    for split in dataset.keys():\n",
    "        config_results[split] = compute_statistics(dataset[split])\n",
    "    results[config] = config_results\n",
    "\n",
    "for config, splits in results.items():\n",
    "    print(f\"Results for configuration: {config}\")\n",
    "    for split, stats in splits.items():\n",
    "        print(f\"  Split: {split}\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for configuration: mlqa.de.de\n",
      "  Split: test\n",
      "    Number of samples: 4517\n",
      "    Total number of tokens: 101868\n",
      "    Average tokens per sample: 22.55213637369936\n",
      "    Average tokens per question: 14.772636705778172\n",
      "    Average tokens per answer: 7.779499667921186\n",
      "    Standard deviation of tokens per sample: 10.541430453552449\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 512\n",
      "    Total number of tokens: 11547\n",
      "    Average tokens per sample: 22.552734375\n",
      "    Average tokens per question: 14.966796875\n",
      "    Average tokens per answer: 7.5859375\n",
      "    Standard deviation of tokens per sample: 10.359417531198043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    question_tokens_list = []\n",
    "    answer_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        question = example['question']\n",
    "        answer = example['answers']['text'][0] \n",
    "        question_tokens = tokenize_and_count(question)\n",
    "        answer_tokens = tokenize_and_count(answer)\n",
    "        total_tokens += question_tokens + answer_tokens\n",
    "        question_tokens_list.append(question_tokens)\n",
    "        answer_tokens_list.append(answer_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_question_tokens = np.mean(question_tokens_list)\n",
    "    average_answer_tokens = np.mean(answer_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([q + a for q, a in zip(question_tokens_list, answer_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per question\": average_question_tokens,\n",
    "        \"Average tokens per answer\": average_answer_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "# Available configurations\n",
    "configs = [ 'mlqa.de.de']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    dataset = load_dataset(\"facebook/mlqa\", config)\n",
    "    config_results = {}\n",
    "    for split in dataset.keys():\n",
    "        config_results[split] = compute_statistics(dataset[split])\n",
    "    results[config] = config_results\n",
    "\n",
    "for config, splits in results.items():\n",
    "    print(f\"Results for configuration: {config}\")\n",
    "    for split, stats in splits.items():\n",
    "        print(f\"  Split: {split}\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for configuration: mlqa.en.en\n",
      "  Split: test\n",
      "    Number of samples: 11590\n",
      "    Total number of tokens: 177089\n",
      "    Average tokens per sample: 15.279465056082831\n",
      "    Average tokens per question: 10.537877480586713\n",
      "    Average tokens per answer: 4.741587575496117\n",
      "    Standard deviation of tokens per sample: 6.07776618660986\n",
      "\n",
      "  Split: validation\n",
      "    Number of samples: 1148\n",
      "    Total number of tokens: 17611\n",
      "    Average tokens per sample: 15.340592334494774\n",
      "    Average tokens per question: 10.774390243902438\n",
      "    Average tokens per answer: 4.566202090592334\n",
      "    Standard deviation of tokens per sample: 6.035163251143683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    question_tokens_list = []\n",
    "    answer_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        question = example['question']\n",
    "        answer = example['answers']['text'][0] \n",
    "        question_tokens = tokenize_and_count(question)\n",
    "        answer_tokens = tokenize_and_count(answer)\n",
    "        total_tokens += question_tokens + answer_tokens\n",
    "        question_tokens_list.append(question_tokens)\n",
    "        answer_tokens_list.append(answer_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_question_tokens = np.mean(question_tokens_list)\n",
    "    average_answer_tokens = np.mean(answer_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([q + a for q, a in zip(question_tokens_list, answer_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per question\": average_question_tokens,\n",
    "        \"Average tokens per answer\": average_answer_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "# Available configurations\n",
    "configs = [ 'mlqa.en.en']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    dataset = load_dataset(\"facebook/mlqa\", config)\n",
    "    config_results = {}\n",
    "    for split in dataset.keys():\n",
    "        config_results[split] = compute_statistics(dataset[split])\n",
    "    results[config] = config_results\n",
    "\n",
    "for config, splits in results.items():\n",
    "    print(f\"Results for configuration: {config}\")\n",
    "    for split, stats in splits.items():\n",
    "        print(f\"  Split: {split}\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for split: train\n",
      "  Number of samples: 104255\n",
      "  Total number of tokens: 19312379\n",
      "  Average tokens per sample: 185.24175339312262\n",
      "  Average tokens per prompt: 180.7788499352549\n",
      "  Average tokens per output: 4.4629034578677285\n",
      "  Standard deviation of tokens per sample: 66.86586606323264\n",
      "\n",
      "Statistics for split: valid\n",
      "  Number of samples: 26064\n",
      "  Total number of tokens: 4838499\n",
      "  Average tokens per sample: 185.63915745856355\n",
      "  Average tokens per prompt: 181.1476749539595\n",
      "  Average tokens per output: 4.491482504604051\n",
      "  Standard deviation of tokens per sample: 68.11135825870062\n",
      "\n",
      "Statistics for split: eval\n",
      "  Number of samples: 11873\n",
      "  Total number of tokens: 2291926\n",
      "  Average tokens per sample: 193.03680619893876\n",
      "  Average tokens per prompt: 188.7009180493557\n",
      "  Average tokens per output: 4.335888149583088\n",
      "  Standard deviation of tokens per sample: 75.52868821632072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################### For pythia model in zero-shot dataset ########################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "\n",
    "# Function to tokenize and count tokens\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "# Function to compute statistics for a dataset split\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    prompt_tokens_list = []\n",
    "    output_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        prompt = example['prompt']\n",
    "        output = example['output']\n",
    "        \n",
    "        prompt_tokens = tokenize_and_count(prompt)\n",
    "        output_tokens = tokenize_and_count(output)\n",
    "        \n",
    "        total_tokens += prompt_tokens + output_tokens\n",
    "        prompt_tokens_list.append(prompt_tokens)\n",
    "        output_tokens_list.append(output_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_prompt_tokens = np.mean(prompt_tokens_list)\n",
    "    average_output_tokens = np.mean(output_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([p + o for p, o in zip(prompt_tokens_list, output_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per prompt\": average_prompt_tokens,\n",
    "        \"Average tokens per output\": average_output_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "# Load the dataset from a given directory\n",
    "def load_dataset_from_path(dataset_path, splits):\n",
    "    dataset = {}\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(dataset_path, f\"{split}.jsonl\")\n",
    "        if os.path.exists(split_path):\n",
    "            with open(split_path, 'r', encoding='utf-8') as f:\n",
    "                dataset[split] = [json.loads(line) for line in f]\n",
    "    return dataset\n",
    "\n",
    "# Define the paths\n",
    "train_val_path = \"/home/IAIS/jdatta/distillm-new/processed_data_pythia70m/squad_v2/full/pythia/\"  \n",
    "eval_path = \"/home/IAIS/jdatta/distillm-new/data/squad_json/\"                \n",
    "\n",
    "# Load datasets\n",
    "train_val_splits = ['train', 'valid']\n",
    "eval_splits = ['eval']\n",
    "\n",
    "train_val_dataset = load_dataset_from_path(train_val_path, train_val_splits)\n",
    "eval_dataset = load_dataset_from_path(eval_path, eval_splits)\n",
    "\n",
    "# Compute statistics\n",
    "results = {}\n",
    "\n",
    "# Training and validation splits\n",
    "for split, data_split in train_val_dataset.items():\n",
    "    results[split] = compute_statistics(data_split)\n",
    "\n",
    "# Evaluation split\n",
    "for split, data_split in eval_dataset.items():\n",
    "    results[split] = compute_statistics(data_split)\n",
    "\n",
    "# Print results\n",
    "for split, stats in results.items():\n",
    "    print(f\"Statistics for split: {split}\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for split: train\n",
      "  Number of samples: 104175\n",
      "  Total number of tokens: 39348648\n",
      "  Average tokens per sample: 377.71680345572355\n",
      "  Average tokens per prompt: 373.247813774898\n",
      "  Average tokens per output: 4.468989680825534\n",
      "  Standard deviation of tokens per sample: 99.39265197669258\n",
      "\n",
      "Statistics for split: valid\n",
      "  Number of samples: 26044\n",
      "  Total number of tokens: 9840261\n",
      "  Average tokens per sample: 377.8321686376901\n",
      "  Average tokens per prompt: 373.36376900629705\n",
      "  Average tokens per output: 4.468399631393027\n",
      "  Standard deviation of tokens per sample: 99.85293082755062\n",
      "\n",
      "Statistics for split: eval\n",
      "  Number of samples: 11773\n",
      "  Total number of tokens: 4647037\n",
      "  Average tokens per sample: 394.71986749341715\n",
      "  Average tokens per prompt: 390.3873269345112\n",
      "  Average tokens per output: 4.332540558905971\n",
      "  Standard deviation of tokens per sample: 109.88386266347925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################### For pythia model in one-shot dataset ########################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "\n",
    "# Function to tokenize and count tokens\n",
    "def tokenize_and_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "# Function to compute statistics for a dataset split\n",
    "def compute_statistics(data_split):\n",
    "    num_samples = 0\n",
    "    total_tokens = 0\n",
    "    prompt_tokens_list = []\n",
    "    output_tokens_list = []\n",
    "    \n",
    "    for example in data_split:\n",
    "        num_samples += 1\n",
    "        prompt = example['prompt']\n",
    "        output = example['output']\n",
    "        \n",
    "        prompt_tokens = tokenize_and_count(prompt)\n",
    "        output_tokens = tokenize_and_count(output)\n",
    "        \n",
    "        total_tokens += prompt_tokens + output_tokens\n",
    "        prompt_tokens_list.append(prompt_tokens)\n",
    "        output_tokens_list.append(output_tokens)\n",
    "    \n",
    "    average_tokens_per_sample = total_tokens / num_samples\n",
    "    average_prompt_tokens = np.mean(prompt_tokens_list)\n",
    "    average_output_tokens = np.mean(output_tokens_list)\n",
    "    std_dev_tokens_per_sample = np.std([p + o for p, o in zip(prompt_tokens_list, output_tokens_list)])\n",
    "    \n",
    "    return {\n",
    "        \"Number of samples\": num_samples,\n",
    "        \"Total number of tokens\": total_tokens,\n",
    "        \"Average tokens per sample\": average_tokens_per_sample,\n",
    "        \"Average tokens per prompt\": average_prompt_tokens,\n",
    "        \"Average tokens per output\": average_output_tokens,\n",
    "        \"Standard deviation of tokens per sample\": std_dev_tokens_per_sample\n",
    "    }\n",
    "\n",
    "# Load the dataset from a given directory\n",
    "def load_dataset_from_path(dataset_path, splits):\n",
    "    dataset = {}\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(dataset_path, f\"{split}.jsonl\")\n",
    "        if os.path.exists(split_path):\n",
    "            with open(split_path, 'r', encoding='utf-8') as f:\n",
    "                dataset[split] = [json.loads(line) for line in f]\n",
    "    return dataset\n",
    "\n",
    "# Define the paths\n",
    "train_val_path = \"/home/IAIS/jdatta/distillm-new/processed_data_pythia70m/squad_1shot/full/pythia/\"  \n",
    "eval_path = \"/home/IAIS/jdatta/distillm-new/data/squad_1shot/\"                \n",
    "\n",
    "# Load datasets\n",
    "train_val_splits = ['train', 'valid']\n",
    "eval_splits = ['eval']\n",
    "\n",
    "train_val_dataset = load_dataset_from_path(train_val_path, train_val_splits)\n",
    "eval_dataset = load_dataset_from_path(eval_path, eval_splits)\n",
    "\n",
    "# Compute statistics\n",
    "results = {}\n",
    "\n",
    "# Training and validation splits\n",
    "for split, data_split in train_val_dataset.items():\n",
    "    results[split] = compute_statistics(data_split)\n",
    "\n",
    "# Evaluation split\n",
    "for split, data_split in eval_dataset.items():\n",
    "    results[split] = compute_statistics(data_split)\n",
    "\n",
    "# Print results\n",
    "for split, stats in results.items():\n",
    "    print(f\"Statistics for split: {split}\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a854e7b768478cbdded73834ec921f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4086d2b64043dba052175ef36d88ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4279fa4b96154d38956eb32cfd752ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mandarjoshi/trivia_qa\",'rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?', 'question_id': 'tc_1', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': [], 'filename': [], 'title': [], 'wiki_context': []}, 'search_results': {'description': ['The Nobel Prize in Literature 1930 Sinclair ... The Nobel Prize in Literature 1930 was awarded to ... nobelprize.org/nobel_prizes/literature/laureates/1930/>', 'Why Dont More Americans Win the Nobel Prize? By . ... When the Nobel Prize in Literature was awarded to Sinclair ... In 1930, Lewis told his Nobel audience that ...', '... Sauk Centre native Sinclair Lewis became the first American to be awarded a Nobel Prize for Literature. ... in 1930, Sauk Centre native Sinclair Lewis became the ...', 'Sinclair Lewis - Nobel Prize in Literature, 1930 (20 books) Type ... Literature Fiction Classics Short Stories Essays American literature Nobel Prize Uploaded: 2015 ...', \"The Nobel Prize in Literature 1930 Sinclair Lewis. ... as janitor of Upton Sinclair's abortive co ... nobelprize.org/nobel_prizes/literature/laureates/1930/lewis ...\", 'Sinclair Lewis Becomes the First American to be Awarded the Nobel Prize for Literature. ... In 1930, Lewis won the Nobel Prize in Literature, ... 1930, Sinclair Lewis ...', 'Download Sinclair Lewis - Nobel Prize in Literature, 1930 ... Sinclair Lewis - Nobel Prize in Literature, 1930 (20 books) seeders: 4. leechers: 3. Download torrent.'], 'filename': ['46/46_46.txt', '194/194_50.txt', '127/127_51.txt', '112/112_52.txt', '36/36_53.txt', '83/83_54.txt', '188/188_55.txt'], 'rank': [0, 4, 5, 6, 7, 8, 9], 'title': ['The Nobel Prize in Literature 1930', 'Why Dont More Americans Win the Nobel Prize? - The New Yorker', 'Sinclair Lewis Wins Nobel Prize  on This Date in Central ...', 'Sinclair Lewis - Nobel Prize in Literature, 1930 (20 books ...', 'Sinclair Lewis - Biographical - Nobel Prize', 'Sinclair Lewis Becomes the First American to be Awarded ...', 'Download Sinclair Lewis - Nobel Prize in Literature, 1930 ...'], 'url': ['http://www.nobelprize.org/nobel_prizes/literature/laureates/1930/', 'http://www.newyorker.com/books/page-turner/why-dont-more-americans-win-the-nobel-prize', 'http://wjon.com/sinclair-lewis-wins-nobel-prize-on-this-date-in-central-minnesota-history/', 'https://piratebay.bid/torrent/11997302/Sinclair_Lewis_-_Nobel_Prize_in_Literature__1930_(20_books)', 'http://www.nobelprize.org/nobel_prizes/literature/laureates/1930/lewis-bio.html', 'https://worldhistoryproject.org/1930/sinclair-lewis-becomes-the-first-american-to-be-awarded-the-nobel-prize-for-literature', 'http://kickasstorrents.ee/sinclair-lewis-nobel-prize-in-literature-1930-20-books-t10761358.html'], 'search_context': ['The Nobel Prize in Literature 1930\\nThe Nobel Prize in Literature 1930\\nSinclair Lewis\\nThe Nobel Prize in Literature 1930\\nSinclair Lewis\\nPrize share: 1/1\\nThe Nobel Prize in Literature 1930 was awarded to Sinclair Lewis \"for his vigorous and graphic art of description and his ability to create, with wit and humour, new types of characters\".\\nPhotos: Copyright  The Nobel Foundation\\nShare this:\\nTo cite this page\\nMLA style: \"The Nobel Prize in Literature 1930\". Nobelprize.org. Nobel Media AB 2014. Web. 18 Jan 2017. <http://www.nobelprize.org/nobel_prizes/literature/laureates/1930/>', 'Why Dont More Americans Win the Nobel Prize? - The New Yorker\\nWhy Dont More Americans Win the Nobel Prize?\\nBy\\n\\xa0\\xa0\\nOctober 8, 2013\\nWhen the Nobel Prize in Literature was awarded to Sinclair Lewis, in 1930, it was the first time in the prizes three-decade history that it had been given to an American. Lewiss acceptance lecture was a not-especially-gracious missive aimed at his critics in the United States. Yet the curmudgeonly writer managed more expansive moments, gesturing toward the historic nature of that years award and remarking upon the state of American literature at the time, and on its status in the world.\\nLewis argued that writing in the U.S. had been stunted in the years after Whitman and Twain, and mostly ignored; only architecture and film were taken seriously as popular arts among Americans. The authors who did manage to attract notice were mostly sentimental and blandly patriotic, while cultural critics, like Lewis himself, who were honest enough to express that the country had not yet produced a civilization good enough to satisfy the deepest wants of human creatures, were disparaged. The American novelist or poet or dramatist or sculptor or painter must work alone, in confusion, unassisted save by his own integrity, Lewis said.\\nThis might have sounded familiar to an audience of European intellectualsthe notion of Americans as either a puerile backwoods clan, in Lewiss phrase, or else a boorish mass of humanity enthralled by industry, science, and high finance. By recognizing Lewis with the Nobel, the committee was at once endorsing his political critiques of his home country, and also marking American literature as having come of age. Lewis noted that the award could have gone to one of his contemporariesWilla Cather or Theodore Dreiser or Eugene ONeillbut he also predicted that future committees would have many talented writers to chose from among a group of young Americans that was hard at work giving the United States a literature worthy of her vastness.\\nSince 1930, ten other Americans have received the Nobel Prize in Literature, including a few whom Lewis mentioned in his lectureONeill, William Faulkner, Ernest Hemingway. Others, whom he couldnt have predictedJohn Steinbeck, Saul Bellow, Toni Morrisonhave become central writers in a national literary canon worthy of the vastness of this, or any other, country. Still othersIsaac Bashevis Singer, Czeslaw Milosz, and Joseph Brodskycame to the United States as adults, and wrote primarily in their native languages, which reflected another step toward cosmopolitanism among American letters. (The work of the other American winner, Pearl Buck, who won the Nobel in 1938, has not aged well, and her award has become a frequently cited example of the committees idiosyncratic choices.) Through the twentieth century, the idea of the American literary scene as an overlooked backwater faded, owing to the artistry of these writers and scores of others, but also because the United States became a haven for exiled Europeans during the Second World War and its Cold War aftermath, and, perhaps most especially, because of the economic dominance of the American publishing industry.\\nNowadays, New York is the worlds publishing capital for books written in English, and American literature has joined film and music as one of the countrys principal artistic exports. And yet, echoes of the intellectual situation that Lewis identified in 1930 can still be heard today. Take the controversy that has attached itself to another high-profile international literary award: the Man Booker Prize. In September, the chairman of the Booker Prize Foundation announced that, beginning in 2014, his organization would no longer limit consideration to English-language submissions from the U.K., Ireland, Zimbabwe, and British Commonwealth countries, but would begin considering any novel written in English that had been published in the U.K. On its face, this seemed to be a modern, egalitarian decision. But the response, at least from some in the U.K., was to complain that the prize would be contaminated by an influx of submissions from the United States. That is a fair point: the judges will surely be seeing a lot of American novels next year, and because publishers are limited in the number of books they can submit, fewer will likely come from Commonwealth countries. It is also perfectly fair to argue, as the novelist Jim Crace has , that the Bookers specific limitations accounted for much of its meaning and relevance. (British writers have noted that they likely wont be eligible for the Pulitzer anytime soon.)\\nPart of the backlash has to do with business, since one of the primary functions of a literary prizeand the long and short lists that precede itis to sell books. And a more crowded international field means that books from the U.K. and the Commonwealth may have less of a chance to receive a Booker bump. There is another business argument, which connects back to what Sinclair Lewis meant when he described America, in 1930, as a land that produces eighty-story buildings, motors by the million, and wheat by the billions of bushels. It was what the English novelist Jeanette Winterson was suggesting when she told the London Evening Standard , This country is so in thrall to America. Were such lapdogs to them, and that will skew things with the judges. Images of Tony Blair following George W. Bush around came to mind, but so, too, did Lewiss Nobel remarks about the brute force of American export capitalism. Americans would win more Bookers because they win more of everything.\\nBehind the complaints about the Booker decision, there was another flavor of criticism, perhaps a kind of Old World snobbery, namely about the quality and nature of American literature itself. The British novelist Philip Hensher faults American novels for their broadness, telling the New York Times , The big novel that speaks to all the world is not at the heart of literary achievement. Some very fine novels seem to speak much more to one culture than another and are rooted in something local. Hensher may be conflating American novels with American blockbuster films, which are often constructed to appeal to global audiences in order to maximize profits. But its tough to imagine any novel speaking to all the world; certainly no recent American examples come to mind. Or, to put it another way, a novelist who begins with the hope of speaking to a global audience is very unlikely to produce a book that resonates with anyone. There is sometimes grumbling that American literary awards tend to value large, sprawling social-commentary novels, but a glance at the list of recent Pulitzer and National Book Award winners makes any kind of generalization seem difficult. Regardless, all good novels, whether epics or miniaturist portraits, whether American or not, are local, to use Henshers word; they are local to the authors consciousness, and to the particular physical and emotional landscapes it contains.\\nPerhaps what offends Hensher is America itself as a setting, as if there is something not meaningfully local about American locales. In a blog post for the Guardian , he laments that the Booker had become Americanized even before it changed its official rules, since three of this years finalistsRuth Ozeki, Jhumpa Lahiri, and NoViolet Bulawayonow live and work in the United States at least part-time. Each of their novels tells a story about other countries through what Hensher dismisses as the reassuring filter of North American suburban culture. Or, as he told the Times: Novels about Indians who leave their exotic homeland and live in New Jersey are fine, but they shouldnt crowd out those who write about their own culture. (Lahiris latest novel, The Lowland, is partly set in Rhode Island.) Hensher writes as if foreign writers ought to be protected from the banality of American suburbiaand world readers must be shielded from any literary output that might result from the mixing of the two. As for the future of the Booker, he writes, the novel written by an Indian, living in India, about India, without reference to his later life in Cincinnati doesnt stand a chance.\\nMany have seen the Nobel Prize in Literature, meanwhile, as a kind of international referendum on American literary hegemony. The prize hasnt been awarded to an American since 1993, when Toni Morrison won. The sniping about years of snubs might just have been chalked up to sour grapes, had it not been for the comments, in 2008, by Horace Engdahl, who was at that time the permanent secretary of the Swedish Academy. The U.S. is too isolated, too insular. They dont translate enough and dont really participate in the big dialogue of literature. That ignorance is restraining, he said. Not everything in the remark was outrageous; American publishers do translate too few books from other languages into Englishjust three per cent of books published each year are translations. Yet his remarks overlooked the fact that more than sixty million Americans speak a primary language other than Englishmeaning that the United States is far from being backwardly monocultural or monolingual. Philip Roths New Jersey is also Junot Dazs New Jersey.\\nCritics in this country responded angrily , to which later Engdahl expressed his surprise, and noted that he had perhaps been speaking too generally. He stepped down as permanent secretary in 2009, and his replacement, Peter Englund, has walked back his predecessors indictment of American writing. But the damage was done, and commentators began to see the Nobel Prize in Literature as being actively denied to American writers, and on the same grounds that American intellectuals have long been dismissed by Europeans. Perhaps the best way to insult an American with aspirations to cosmopolitanism is to call him and his fellows ignorant rustics, functional only in English and kept safely away from real intellectual rigor and debate by geographical isolation, local peace, and relative material abundance. The Swedes had decided that we were, as Sinclair Lewis remarked back in 1930, still a puerile backwoods clan.\\nWell, a well-lettered American might ask, Which is it? Are we too disengaged with the world to be taken seriously, or else too deeply engaged with it to be distinct? In 1930, Lewis told his Nobel audience that his home country was coming out  of the stuffiness of safe, sane, and incredibly dull provincialism. He was right, but, all these years later, he might be surprised to hear European intellectuals still saying those things about its literature.\\nIllustration by Maximilian Bode.', \"Sinclair Lewis Wins Nobel Prize  on This Date in Central Minnesota History\\nSinclair Lewis Wins Nobel Prize  on This Date in Central Minnesota History\\nBy Jim Maurice December 10, 2011 7:30 AM\\nSinclair Lewis (Stearns History Museum)\\nSAUK CENTRE  December 10th, 1930  Sauk Centre native, Sinclair Lewis, receives the Nobel Prize for Literature\\nOn this date, December 10th, in 1930, Sauk Centre native Sinclair Lewis became the first American to be awarded a Nobel Prize for Literature.\\nBorn in 1885 in the village of Sauk Centre, Lewis was the third and youngest son of Edwin and Emma Lewis. Young Sinclair Lewis was not like his two older brothers who excelled in sports; Sinclair preferred reading books to playing sports. This atypical personality caused Lewis to be lonely through much of his growing-up years. At one point, a 13 year-old Lewis attempted to run away from home and become a drummer in the Spanish-American War.\\nLewis' boyhood home in Sauk Centre in 1970 (Stearns History Museum)\\nLewis attended Oberlin Academy and then Yale. It was at Yale that he first became truly published. Lewis wrote for the Yale Literary Magazine, turning out poetry and short stories, and later becoming editor. As Lewis continued to write, his works became better-known. Before long, magazines were buying Lewis stories, and Jack London even bought a plot from him.\\nLewis struggled with the personal side of life from the time he was a boy, and his problems continued throughout his two marriages. He first married a magazine editor in 1914. The couple had one son, and divorced in 1925. Lewis second marriage to a newspaper columnist in 1928 resulted in another son, and lasted until 1942.\\nLewis reflected back on his time growing up to find most of his inspiration for his best-known work, Main Street, which was published in 1920. Lewis had hoped to sell 25,000 copies of Main Street, but sales surpassed 150,000 copies in the first few years alone. Lewis went on to write several more books, including Babbitt in 1922 which helped win Lewis his Nobel Prize for Literature.\\nAfter receiving his Nobel Prize, Lewis continued writing, and produced eleven more works. Ten of those works were published while he was still alive, the eleventh would be published after his death in 1951. Lewis died in Rome, due to advanced alcoholism, a problem that had troubled his life since the mid 1930s. Lewis remains were buried in his hometown of Sauk Centre, where his boyhood home still stands.\\nThanks to the Stearns History Museum , and their volunteer Spencer Brown, for their help with our series, This Date in Central Minnesota History on WJON.\", 'Sinclair Lewis - Nobel Prize in Literature, 1930 (20 books) (download torrent) - TPB\\n\\xa0Get this torrent\\n(Problems with magnets links are fixed by upgrading your torrent client !)\\nSINCLAIR LEWIS (1885-1951) was an American novelist, short-story writer, and playwright.  In 1930, he became the first American writer to receive the Nobel Prize in Literature, which was awarded \"for his vigorous and graphic art of description and his ability to create, with wit and humor, new types of characters.\"  His works are known for their insightful and critical views of American capitalism and materialism between the wars.  He is also respected for his strong characterizations of modern working women.  H.L. Mencken wrote of him, \"[If] there was ever a novelist among us with an authentic call to the trade ... it is this red-haired tornado from the Minnesota wilds.\"  His first novel, OUR MR. WRENN (1914) is a gently satiric account of a meek New York clerk traveling in Europe.  Lewis wrote four more novels and achieved only modest success.  But MAIN STREET (1920) caused a sensation and brought him immediate fame.  The book is a withering satire on the dullness and lack of culture that exist in a \"typical\" American small town, and the narrow-mindedness and self-satisfaction of its inhabitants.  BABBITT (1922) focuses even more effectively Lewis\\' idea of a \"typical\" small city businessman, George F. Babbitt.  The novel describes the futile attempt of its central character to break loose from the confining life of a \"solid American citizen\" -- a middle-class, middle-aged realtor, civic booster, and club joiner.  Possibly no two works of literature did more to make Americans aware of the limitations of their national life and culture than did MAIN STREET and BABBITT.  With a sharp, satiric eye and a superb gift for mimicry, Lewis continued to examine other aspects of what he considered national inadequacy.  ARROWSMITH (1925) describes the frustrations of an idealistic young doctor in conflict with corruption, jealousy, meanness, and prejudice.  The novel won the 1926 Pulitzer Prize, which Lewis declined because he felt that it was not awarded for literary merit but for the best presentation of \"wholesome\" American life.  Lewis closed out the decade with DODSWORTH (1929), a novel about the most affluent and successful members of American society.  He portrayed them as leading essentially pointless lives in spite of great wealth and advantages.  After winning the Nobel Prize in 1930, Lewis wrote eleven more novels.  The best remembered is IT CAN\\'T HAPPEN HERE (1935), a novel about the election of a fascist to the American presidency.  In addition to his major novels, this torrent includes a selection of Lewis\\' short stories (I\\'M A STRANGER HERE MYSELF) and essays (THE MAN FROM MAIN STREET), the latter of which reproduces the text of his Nobel Prize address.   The following books are in PDF or ePUB format as indicated:  * ARROWSMITH (HarperPerennial, 2012) -- ePUB  * BABBITT (Bantam Classics, 1998).  Introduction by John Wickersham. -- ePUB  * BABBITT (Barnes & Noble, 2005).  Introduction and Notes by Kenneth Krauss. -- ePUB  * BABBITT (HarperPerennial, 2012) -- ePUB  * BABBITT (Oxford World\\'s Classics, 2010).  Edited with an Introduction and Notes by Gordon Hutner. -- PDF  * BETHEL MERRIDAY (Jonathan Cape, 1940) -- PDF  * DODSWORTH (HarperPerennial, 2012) -- ePUB  * FREE AIR (HarperPerennial, 2012) -- ePUB  * GIDEON PLANISH (Jonathan Cape, 1943) -- PDF  * THE GOD-SEEKER (Popular Library, 1948) -- PDF  * I\\'M A STRANGER HERE MYSELF & OTHER STORIES (Dell, 1962).  Selected by Mark Schorer. -- PDF  * IT CAN\\'T HAPPEN HERE (Signet, 2014).  Introduction by Michael Meyer and a New Afterword by Gary Scharnhorst. -- ePUB  * MAIN STREET (Barnes & Noble, 2003).  Introduction and Notes by Brooke Allen. -- ePUB  * MAIN STREET (HarperPerennial, 2012) -- ePUB  * MAIN STREET (Modern Library, 1999).  Introduction by Carol Kennicott. -- ePUB  * THE MAN FROM MAIN STREET: Selected Essays & Other Writings, 1904-1950 (Pocket Books, 1963).  Edited by Harry E. Maule and Melville H. Cane. -- PDF  * OUR MR. WRENN (Grosset & Dunlap, 1914) -- PDF  * PREMIUM COLLECTION: 7 Novels: Our Mr. Wrenn / The Trail of the Hawk / The Job / The Innocents / Free Air / Main Street / Babbitt (Timeless Wisdom, 2014) -- ePUB  * THE PRODIGAL PARENTS (Doubleday, 1934) -- PDF  * WORK OF ART (Collier, 1934) -- PDF  _____________________________________________________________________________ >> CONTACT ME You may reach me with comments, suggestions, requests, error reports, etc., at TPB\\'s forum, SuprBay (you will need to register an account): https://pirates-forum.org/User-workerbee >> PLEASE HELP TO SEED! If you like these books and want others to have access to them, please consider seeding for as long as you can.  The more you seed, the longer the torrent will live, and the easier it will be for me to upload new content.  Thank you!', \"Sinclair Lewis - Biographical\\nSinclair Lewis\\nThe Nobel Prize in Literature 1930\\nSinclair Lewis\\nShare this:\\nSinclair Lewis - Biographical\\nTo recount my life for the Nobel   Foundation, I would like to present it as possessing some   romantic quality, some unique character, like Kipling 's early adventures in India, or Bernard Shaw 's leadership in the   criticism of British arts and economics. But my life, aside from   such youthful pranks as sailing on cattleships from America to   England during university vacations, trying to find work in   Panama during the building of the Canal, and serving for two   months as janitor of Upton Sinclair's abortive co-operative   colony, Helicon Hall, has been a rather humdrum chronicle of much   reading, constant writing, undistinguished travel  la   tripper, and several years of comfortable servitude as an   editor.\\nI was born in a prairie village in that most Scandinavian part of   America, Minnesota, the son of a country doctor, in 1885. Until I   went East to Yale   University I attended the ordinary public school, along with   many Madsens, Olesons, Nelsons, Hedins, Larsons. Doubtless it was   because of this that I made the hero of my second book, The   Trail of the Hawk, a Norwegian, and Gustaf Sondelius, of Arrowsmith, a Swede - and to me, Dr. Sondelius is the   favorite among all my characters.\\nOf Carl Ericson of The Trail of the Hawk, I wrote -back in   1914, when I was working all day as editor for the George H.   Doran Publishing Company, and all evening trying to write novels   - as follows:\\nHis carpenter father had come from Norway, by way of   steerage and a farm in Wisconsin, changing his name (to   Americanize it) from Ericsen... Carl was second-generation   Norwegian; American-born, American in speech, American in   appearance, save for his flaxen hair and china-blue eyes... When   he was born the typical Americans of earlier stocks   had moved to city palaces or were marooned on run-down farms. It   was Carl Ericson, not a Trowbridge or a Stuyvesant or a Lee or a   Grant, who was the typical American of his period.   It was for him to carry on the American destiny of extending the   Western horizon; his to restore the wintry Pilgrim virtues and   the exuberant October, partridge-drumming days of Daniel Boone;   then to add, in his own or another generation, new American   aspirations for beauty.\\nMy university days at Yale were undistinguished save for   contributions to the Yale Literary Magazine. It may be   interesting to say that these contributions were most of them   reeking with a banal romanticism; that an author who was later to   try to present ordinary pavements trod by real boots should   through university days have written nearly always of Guinevere   and Lancelot - of weary bitterns among sad Irish reeds - of   story-book castles with troubadours vastly indulging in wine, a   commodity of which the author was singularly ignorant. What the   moral is, I do not know. Whether imaginary castles at nineteen   lead always to the sidewalks of Main Street at thirty-five, and   whether the process might be reversed, and whether either of them   is desirable, I leave to psychologists.\\nI drifted for two years after college as a journalist, as a   newspaper reporter in Iowa and in San Francisco, as - incredibly   - a junior editor on a magazine for teachers of the deaf, in   Washington, D.C. The magazine was supported by Alexander Graham   Bell, inventor of the telephone. What I did not know about   teaching the deaf would have included the entire subject, but   that did not vastly matter, as my position was so insignificant   that it included typing hundreds of letters every week begging   for funds for the magazine and, on days when the Negro janitress   did not appear, sweeping out the office.\\nDoubtless this shows the advantages of a university education,   and it was further shown when at the age of twenty-five I managed   to get a position in a New York publishing house at all of   fifteen dollars a week. This was my authentic value on the labor   market, and I have always uncomfortably suspected that it would   never have been much higher had I not, accidentally, possessed   the gift of writing books which so acutely annoyed American   smugness that some thousands of my fellow citizens felt they must   read these scandalous documents, whether they liked them or   not.\\nFrom that New York position till the time five years later when I   was selling enough short stories to the magazines to be able to   live by free-lancing, I had a series of typical white-collar,   unromantic, office literary jobs with two publishing houses, a   magazine (Adventure), and a newspaper syndicate, reading   manuscripts, writing book advertising, writing catalogues,   writing uninspired book reviews - all the carpentry and plumbing   of the city of letters. Nor did my first five novels rouse the   slightest whispers: Our Mr. Wrenn, The Trail of the   Hawk, The Job, The Innocents, and Free   Air they were called, published between 1914 and 1919, and   all of them dead before the ink was dry. I lacked sense enough to   see that, after five failures, I was foolish to continue   writing.\\nMain Street, published late in 1920, was my first novel to   rouse the embattled peasantry and, as I have already hinted, it   had really a success of scandal. One of the most treasured   American myths had been that all American villages were   peculiarly noble and happy, and here an American attacked that   myth. Scandalous. Some hundreds of thousands read the book with   the same masochistic pleasure that one has in sucking an aching   tooth.\\nSince Main Street, the novels have been Babbitt (1922); Arrowsmith (1925); Mantrap (1926); Elmer   Gantry (1927); The Man Who Knew Coolidge (1928); and Dodsworth (1929). The next novel, yet unnamed, will   concern idealism in America through three generations, from 1818   till 1930-an idealism which the outlanders who call Americans dollar-chasers do not understand. It will presumably   be published in the autumn of 1932, and the author's chief   difficulty in composing it is that, after having received the   Nobel Prize, he longs to write better than he can.\\nI was married, in England, in 1928, to Dorothy Thompson, an   American who had been the Central European correspondent and chef de bureau of the New York Evening Post. My first   marriage, to Grace Hegger, in New York, in 1914, had been   dissolved.\\nDuring these years of novelwriting since 1915, I have lived a   quite unromantic and unstirring life. I have travelled much; on   the surface it would seem that one who during these fifteen years   had been in forty states of the United States, in Canada, Mexico,   England, Scotland, France, Italy, Sweden, Germany, Austria,   Czechoslovakia, Jugoslavia, Greece, Switzerland, Spain, the West   Indies, Venezuela, Colombia, Panama, Poland, and Russia must have   been adventurous. That, however, would be a typical error of   biography. The fact is that my foreign travelling has been a   quite uninspired recreation, a flight from reality. My real   travelling has been sitting in Pullman smoking cars, in a   Minnesota village, on a Vermont farm, in a hotel in Kansas City   or Savannah, listening to the normal daily drone of what are to   me the most fascinating and exotic people in the world - the   Average Citizens of the United States, with their friendliness to   strangers and their rough teasing, their passion for material   advancement and their shy idealism, their interest in all the   world and their boastful provincialism - the intricate   complexities which an American novelist is privileged to   portray.\\nAnd nowadays, at forty-six, with my first authentic home - a farm   in the pastoral state of Vermont - and a baby born in June 1930,   I am settled down to what I hope to be the beginning of a   novelist's career. I hope the awkward apprenticeship with all its   errors is nearly done.\\n\\xa0\\nBiographical note on Sinclair   Lewis\\nSinclair Lewis (1885-1951) continued to be   a prolific writer, but none of his later writings equalled the   success or stature of his chiefworks of the twenties. After his   divorce from his second wife in 1942, Sinclair Lewis lived   chiefly in Europe. His later novels include Ann Vickers (I933), It Can't Happen Here (1935), The Prodigal   Parents (1938), Gideon Planish (1943), Cass   Timberlane (1945), Kingsblood Royal ( 1947), The   God-Seeker (1949), and World So Wide (1951). From   Main Street to Stockholm: Letters of Sinclair Lewis 1919-1930 was published in 1952, one year after his death in Rome.\\nFrom Nobel Lectures , Literature 1901-1967, Editor Horst Frenz, Elsevier Publishing Company, Amsterdam, 1969\\nThis autobiography/biography was written    at the time of the award and first     published in the book series Les      Prix Nobel .      It was later edited and republished in Nobel Lectures .  To cite this document, always state the source as shown above.\\n\\xa0\\nSinclair Lewis died on January 10, 1951.\", 'Sinclair Lewis Becomes the First American to be Awarded the Nobel Prize for Literature | World History Project\\n1930\\nSinclair Lewis Becomes the First American to be Awarded the Nobel Prize for Literature\\nIn 1930, Lewis won the Nobel Prize in Literature, the first writer from the United States to receive the award.\\nIn the Swedish Academy\\'s presentation speech, special attention was paid to Babbitt. In his Nobel Lecture, Lewis praised Theodore Dreiser, Willa Cather, Ernest Hemingway, and other contemporaries, but also lamented that \"in America most of us  not readers alone, but even writers  are still afraid of any literature which is not a glorification of everything American, a glorification of our faults as well as our virtues,\" and that America is \"the most contradictory, the most depressing, the most stirring, of any land in the world today.\" He also offered a profound criticism of the American literary establishment: \"Our American professors like their literature clear and cold and pure and very dead.\"\\nSource: Wikipedia Added by: Colin Harris\\nIn 1925 Lewis divorced from his first wife and married three years later Dorothy Thompson, a newspaper correspondent, with whom he traveled to London, Berlin, Vienna, and Moscow. At that time Lewis was drinking heavily, and managed to offend most of his friends. Theodore Dreiser, the other American finalist for the Nobel Prize, was bitterly disappointed, when Lewis won the award. Hwmingway said that the prize should have gone to Ezra Pound or James Joyce.\\nSource: \\'(Harry) Sinclair Lewis (1885-1951)\\'; Petri Liukkonen, http://kirjasto.sci.fi/slewis.htm Added by: Colin Harris\\nOn the morning of November 5, 1930, Sinclair Lewis got up very late, and he was wandering about his rented Westport house when the telephone rang and an excited voice with a Swedish accent announced to him that he had been awarded the Nobel Prize in literature. The voice was that of a Swedish newspaper correspondent in New York who had managed to track down Lewis for the Swedish Embassy, but Lewis thought that it was the voice of his friend Ferd Reyher, who liked to do imitations and play jokes. Oh, yeah? he replied. You dont say! Listen, Ferd, I can say that better than you. Your Swedish accents no good. Ill repeat it to you. And he repeated it, You haf de Nobel Brize, and more. The bewildered Swede protested in vain and finally called an American to the telephone to confirm the news. Lewis fell into a chair.', 'Download Sinclair Lewis - Nobel Prize in Literature, 1930 (20 books) Torrent - kickasstorrents\\nLewis, Sinclair - Work of Art (Collier, 1934).pdf\\n5.13 MB\\nDescription\\nSINCLAIR LEWIS (1885-1951) was an American novelist, short-story writer, and playwright. In 1930, he became the first American writer to receive the Nobel Prize in Literature, which was awarded \"for his vigorous and graphic art of description and his ability to create, with wit and humor, new types of characters.\"\\nHis works are known for their insightful and critical views of American capitalism and materialism between the wars. He is also respected for his strong characterizations of modern working women. H.L. Mencken wrote of him, \"[If] there was ever a novelist among us with an authentic call to the trade ... it is this red-haired tornado from the Minnesota wilds.\"\\nHis first novel, OUR MR. WRENN (1914) is a gently satiric account of a meek New York clerk traveling in Europe. Lewis wrote four more novels and achieved only modest success. But MAIN STREET (1920) caused a sensation and brought him immediate fame. The book is a withering satire on the dullness and lack of culture that exist in a \"typical\" American small town, and the narrow-mindedness and self-satisfaction of its inhabitants.\\nBABBITT (1922) focuses even more effectively Lewis\\' idea of a \"typical\" small city businessman, George F. Babbitt. The novel describes the futile attempt of its central character to break loose from the confining life of a \"solid American citizen\" -- a middle-class, middle-aged realtor, civic booster, and club joiner. Possibly no two works of literature did more to make Americans aware of the limitations of their national life and culture than did MAIN STREET and BABBITT.\\nWith a sharp, satiric eye and a superb gift for mimicry, Lewis continued to examine other aspects of what he considered national inadequacy. ARROWSMITH (1925) describes the frustrations of an idealistic young doctor in conflict with corruption, jealousy, meanness, and prejudice. The novel won the 1926 Pulitzer Prize, which Lewis declined because he felt that it was not awarded for literary merit but for the best presentation of \"wholesome\" American life.\\nLewis closed out the decade with DODSWORTH (1929), a novel about the most affluent and successful members of American society. He portrayed them as leading essentially pointless lives in spite of great wealth and advantages. After winning the Nobel Prize in 1930, Lewis wrote eleven more novels. The best remembered is IT CAN\\'T HAPPEN HERE (1935), a novel about the election of a fascist to the American presidency.\\nIn addition to his major novels, this torrent includes a selection of Lewis\\' short stories (I\\'M A STRANGER HERE MYSELF) and essays (THE MAN FROM MAIN STREET), the latter of which reproduces the text of his Nobel Prize address.\\nThe following books are in PDF or ePUB format as indicated:\\n* ARROWSMITH (HarperPerennial, 2012) -- ePUB\\n* BABBITT (Bantam Classics, 1998). Introduction by John Wickersham. -- ePUB\\n* BABBITT (Barnes & Noble, 2005). Introduction and Notes by Kenneth Krauss. -- ePUB\\n* BABBITT (HarperPerennial, 2012) -- ePUB\\n* BABBITT (Oxford World\\'s Classics, 2010). Edited with an Introduction and Notes by Gordon Hutner. -- PDF\\n* BETHEL MERRIDAY (Jonathan Cape, 1940) -- PDF\\n* DODSWORTH (HarperPerennial, 2012) -- ePUB\\n* FREE AIR (HarperPerennial, 2012) -- ePUB\\n* GIDEON PLANISH (Jonathan Cape, 1943) -- PDF\\n* THE GOD-SEEKER (Popular Library, 1948) -- PDF\\n* I\\'M A STRANGER HERE MYSELF & OTHER STORIES (Dell, 1962). Selected by Mark Schorer. -- PDF\\n* IT CAN\\'T HAPPEN HERE (Signet, 2014). Introduction by Michael Meyer and a New Afterword by Gary Scharnhorst. -- ePUB\\n* MAIN STREET (Barnes & Noble, 2003). Introduction and Notes by Brooke Allen. -- ePUB\\n* MAIN STREET (HarperPerennial, 2012) -- ePUB\\n* MAIN STREET (Modern Library, 1999). Introduction by Carol Kennicott. -- ePUB\\n* THE MAN FROM MAIN STREET: Selected Essays & Other Writings, 1904-1950 (Pocket Books, 1963). Edited by Harry E. Maule and Melville H. Cane. -- PDF\\n* OUR MR. WRENN (Grosset & Dunlap, 1914) -- PDF\\n* PREMIUM COLLECTION: 7 Novels: Our Mr. Wrenn / The Trail of the Hawk / The Job / The Innocents / Free Air / Main Street / Babbitt (Timeless Wisdom, 2014) -- ePUB\\n* THE PRODIGAL PARENTS (Doubleday, 1934) -- PDF\\n* WORK OF ART (Collier, 1934) -- PDF']}, 'answer': {'aliases': ['(Harry) Sinclair Lewis', 'Harry Sinclair Lewis', 'Lewis, (Harry) Sinclair', 'Grace Hegger', 'Sinclair Lewis'], 'normalized_aliases': ['grace hegger', 'lewis harry sinclair', 'harry sinclair lewis', 'sinclair lewis'], 'matched_wiki_entity_name': '', 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'sinclair lewis', 'type': 'WikipediaEntity', 'value': 'Sinclair Lewis'}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataset[split]:\n\u001b[0;32m---> 21\u001b[0m     context_tokens \u001b[38;5;241m=\u001b[39m count_tokens(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m     question_tokens \u001b[38;5;241m=\u001b[39m count_tokens(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m     answer_tokens \u001b[38;5;241m=\u001b[39m count_tokens(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'context'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', trust_remote_code=True)\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "stats = {\n",
    "    'split': [],\n",
    "    'num_samples': [],\n",
    "    'total_tokens': [],\n",
    "    'average_tokens_per_sample': []\n",
    "}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    num_samples = len(dataset[split])\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for sample in dataset[split]:\n",
    "        context = ' '.join(sample['search_results']['description']) + ' ' + ' '.join(sample['search_results']['search_context'])\n",
    "        question_tokens = count_tokens(sample['question'])\n",
    "        context_tokens = count_tokens(context)\n",
    "        answer_tokens = count_tokens(sample['answer']['value']) if 'answer' in sample and 'value' in sample['answer'] else 0\n",
    "        \n",
    "        total_tokens += context_tokens + question_tokens + answer_tokens\n",
    "    \n",
    "    average_tokens = total_tokens / num_samples if num_samples else 0\n",
    "    \n",
    "    stats['split'].append(split)\n",
    "    stats['num_samples'].append(num_samples)\n",
    "    stats['total_tokens'].append(total_tokens)\n",
    "    stats['average_tokens_per_sample'].append(average_tokens)\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023d214605f54a27946db25a258cbe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/63.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72dd1ec9c5147b4b20d6c0773c7c316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/80069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4615997809d46c7926c2c38cead2fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9927 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/mlqa\",'mlqa.en.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Architektonisch, die Schule hat einen katholischen Charakter. Die goldene Kuppel des Main Building ist eine goldene Statue der Jungfrau Maria. Direkt vor dem Haupt-Gebude und vor der Tr, ist eine Kupfer-Statue von Christus mit Armen erhobenen mit der Legende Venite ad Me Omnes. Neben dem Main Building befindet sich die Basilika des Heiligen Herzens. Direkt hinter der Basilika befindet sich die Grotte, ein Marian-Ort des Gebets und der Reflexion. Es ist eine Replik der Grotte in Lourdes, Frankreich, wo die Jungfrau Maria angeblich 1858. zu Saint Bernadette Soubirous erschienen ist. Am Ende der Haupt-Fahrt (und in einer direkten Linie, die durch 3 Statuen und die Gold Kuppel verbindet), ist eine einfache, moderne Stein Statue von Mary.', 'question': 'Wem ist die Jungfrau Maria angeblich 1858 in Lourdes Frankreich erschienen?', 'answers': {'answer_start': [547], 'text': ['Saint Bernadette Soubirous']}, 'id': '5733be284776f41900661182'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        split  num_samples  total_tokens  average_tokens_per_sample\n",
      "0       train        80069      23827329                 297.584945\n",
      "1  validation         9927       3023402                 304.563514\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', trust_remote_code=True)\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "stats = {\n",
    "    'split': [],\n",
    "    'num_samples': [],\n",
    "    'total_tokens': [],\n",
    "    'average_tokens_per_sample': []\n",
    "}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    num_samples = len(dataset[split])\n",
    "    total_tokens = sum(\n",
    "        count_tokens(sample['context']) + count_tokens(sample['question']) + count_tokens(sample['answers']['text'][0])\n",
    "        for sample in dataset[split]\n",
    "    )\n",
    "    average_tokens = total_tokens / num_samples\n",
    "    \n",
    "    stats['split'].append(split)\n",
    "    stats['num_samples'].append(num_samples)\n",
    "    stats['total_tokens'].append(total_tokens)\n",
    "    stats['average_tokens_per_sample'].append(average_tokens)\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Dataset    Lines      Words  Characters\n",
      "0           doc_check    13110    8019547    71710420\n",
      "1             grascco       62      33753      300219\n",
      "2          guidelines      789    8919942    85503512\n",
      "3                kres        3         60        2121\n",
      "4      oscar_med_2040  1120689  580976204  5279432594\n",
      "5      oscar_med_2301     1871     513132     4895319\n",
      "6          phd_theses     6567   97073431   750519010\n",
      "7     phd_theses_sudo     6567   93827818   758790213\n",
      "8    pubmed_abstracts     4194     771699     8782598\n",
      "9      springer_jsons    13963   44733736   366504420\n",
      "10       ufal_medizin  1928645   86965612  1667292240\n",
      "11  wikipedia_medizin    70482   41870648   382511745\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "datasets = {\n",
    "    \"doc_check\": \"/data/share/project/smart_hospital/medical_dataset/doc_check/03_filtered_quality/dataset.jsonl\",\n",
    "    \"grascco\": \"/data/share/project/smart_hospital/medical_dataset/grascco/03_filtered_quality/dataset.jsonl\",\n",
    "    \"guidelines\": \"/data/share/project/smart_hospital/medical_dataset/guidelines/03_filtered_quality/dataset.jsonl\",\n",
    "    \"kres\": \"/data/share/project/smart_hospital/medical_dataset/kres/03_filtered_quality/dataset.jsonl\",\n",
    "    \"oscar_med_2040\": \"/data/share/project/smart_hospital/medical_dataset/oscar_med_2040/03_filtered_quality/dataset.jsonl\",\n",
    "    \"oscar_med_2301\": \"/data/share/project/smart_hospital/medical_dataset/oscar_med_2301/03_filtered_quality/dataset.jsonl\",\n",
    "    \"phd_theses\": \"/data/share/project/smart_hospital/medical_dataset/phd_theses/03_filtered_quality/dataset.jsonl\",\n",
    "    \"phd_theses_sudo\": \"/data/share/project/smart_hospital/medical_dataset/phd_theses_sudo/03_filtered_quality/dataset.jsonl\",\n",
    "    \"pubmed_abstracts\": \"/data/share/project/smart_hospital/medical_dataset/pubmed_abstracts/03_filtered_quality/dataset.jsonl\",\n",
    "    \"springer_jsons\": \"/data/share/project/smart_hospital/medical_dataset/springer_jsons/03_filtered_quality/dataset.jsonl\",\n",
    "    \"ufal_medizin\": \"/data/share/project/smart_hospital/medical_dataset/ufal_medizin/03_filtered_quality/dataset.jsonl\",\n",
    "    \"wikipedia_medizin\": \"/data/share/project/smart_hospital/medical_dataset/wikipedia_medizin/03_filtered_quality/dataset.jsonl\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "def run_wc(command, path):\n",
    "    result = subprocess.run(command.split() + [path], capture_output=True, text=True)\n",
    "    return int(result.stdout.split()[0])\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    lines = run_wc('wc -l', path)\n",
    "    words = run_wc('wc -w', path)\n",
    "    characters = run_wc('wc -c', path)\n",
    "    results.append({\"Dataset\": name, \"Lines\": lines, \"Words\": words, \"Characters\": characters})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"squad\": {\n",
    "    \"file_name\": \"squad_demo.json\",\n",
    "    \"formatting\": \"alpaca\",\n",
    "    \"ranking\": false,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\",\n",
    "      \"response\": \"answers\",\n",
    "      \"history\": \"context\"\n",
    "    }\n",
    "  },"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
